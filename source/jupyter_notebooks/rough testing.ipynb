{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from random import choice, sample\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, Lambda, Dense, Flatten,MaxPooling2D,Activation, Dropout, BatchNormalization,  GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Subtract,Multiply\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from skimage.io import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from random import sample, choice\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "train_file_path = \"data/train/\"\n",
    "train_relationships_path = \"../csv_files/train_relationships.csv\"\n",
    "#the validation set will be family members F09...\n",
    "validation_set = \"F09\"\n",
    "\n",
    "#get all the images\n",
    "all_images = glob(train_file_path + \"*/*/*.jpg\")\n",
    "\n",
    "#seperate the train and validation sets\n",
    "train_images = [img for img in all_images if validation_set not in img]\n",
    "val_images = [img for img in all_images if validation_set in img]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12379\n"
     ]
    }
   ],
   "source": [
    "print(len(all_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_person_to_images_map = defaultdict(list)\n",
    "\n",
    "# ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\n",
    "\n",
    "# for x in train_images:\n",
    "#     train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "\n",
    "# val_person_to_images_map = defaultdict(list)\n",
    "\n",
    "# for x in val_images:\n",
    "#     val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary with key=family member and value=list of pictures of family member\n",
    "def fam_mem_pics(all_images):\n",
    "    #create dictionary\n",
    "    fam_member_dict = {}\n",
    "    #create list for member pictures\n",
    "    fam_member_lst = []\n",
    "    i = 0\n",
    "    while i < len(all_images)-1:\n",
    "        #get the family member as key\n",
    "        split_path = all_images[i].split('/')\n",
    "        key = split_path[2]+'/'+split_path[3]\n",
    "        #check if picture is about the same fmaily member\n",
    "        same_member = True\n",
    "\n",
    "        while same_member:\n",
    "            #check if same family member\n",
    "            if key in all_images[i]:\n",
    "                #append the image to the family member list\n",
    "                fam_member_lst.append(all_images[i])\n",
    "                \n",
    "            else:\n",
    "                #changed family member\n",
    "                same_member = False\n",
    "            \n",
    "            #update the counter to not go out of bounds\n",
    "            if (i+1) < len(all_images):  \n",
    "                i += 1\n",
    "            else:\n",
    "                #break if out of bounds\n",
    "                break\n",
    "                \n",
    "        #check if key is already in dictionary - just in case the images are out of order\n",
    "        if key in fam_member_dict.keys():\n",
    "            #combine list\n",
    "            fam_member_dict[key] = fam_member_dict[key] + fam_member_lst\n",
    "        else: \n",
    "            #create key=family member and value= list of family member's pictures\n",
    "            fam_member_dict[key] = fam_member_lst\n",
    "        \n",
    "        fam_member_lst = []\n",
    "        \n",
    "    #return the member_dictionary \n",
    "    return fam_member_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a family member image dictionary\n",
    "train_person_to_images_map = fam_mem_pics(train_images)\n",
    "val_person_to_images_map = fam_mem_pics(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_person_to_images_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_person_to_images_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the raltionships dataframe\n",
    "df = pd.read_csv(\"csv_files/train_relationships.csv\")\n",
    "\n",
    "#get isolate the the family/member portions to compare\n",
    "all_family_members = []\n",
    "for img in all_images:\n",
    "    split_path = img.split('/')\n",
    "    all_family_members.append(split_path[2]+'/'+split_path[3])\n",
    "\n",
    "#some relationships are not present within the relationship csv\n",
    "#remove them by only keeping the ones mentioned in the dataset\n",
    "df = df[df['p1'].isin(all_family_members)]\n",
    "df = df[df['p2'].isin(all_family_members)]\n",
    "\n",
    "#seperate the training and validation labels\n",
    "train = df[~df['p1'].str.contains(validation_set)]\n",
    "val = df[df['p1'].str.contains(validation_set)]\n",
    "\n",
    "#only keep the values in the dictionary keys\n",
    "train = train[train['p1'].isin(list(train_person_to_images_map.keys()))]\n",
    "val = val[val['p1'].isin(list(val_person_to_images_map.keys()))]\n",
    "\n",
    "train = train[train['p2'].isin(list(train_person_to_images_map.keys()))]\n",
    "val = val[val['p2'].isin(list(val_person_to_images_map.keys()))]\n",
    "\n",
    "#turn dataframes to tuples to get labels\n",
    "train = list(zip(train['p1'].values, train['p2'].values))\n",
    "val = list(zip(val['p1'].values, val['p2'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('F0002/MID1', 'F0002/MID3'),\n",
       " ('F0002/MID2', 'F0002/MID3'),\n",
       " ('F0005/MID1', 'F0005/MID2'),\n",
       " ('F0005/MID3', 'F0005/MID2'),\n",
       " ('F0009/MID1', 'F0009/MID4'),\n",
       " ('F0009/MID1', 'F0009/MID3'),\n",
       " ('F0009/MID1', 'F0009/MID2'),\n",
       " ('F0009/MID1', 'F0009/MID6'),\n",
       " ('F0009/MID2', 'F0009/MID4'),\n",
       " ('F0009/MID2', 'F0009/MID6')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train))\n",
    "train[:10] #labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_img(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = np.array(img).astype(np.float)\n",
    "    return preprocess_input(img)\n",
    "\n",
    "#create a generator\n",
    "def gen(family_member_labels, family_member_map, batch_size=16):\n",
    "    while True:\n",
    "        #sample from half of true labels - don't repeat grab\n",
    "        half_batch_size = int(batch_size/2)\n",
    "        batch_family_members = sample(family_member_labels, half_batch_size)\n",
    "        #all these should be of label 1\n",
    "        label = np.ones(len(batch_family_members))\n",
    "\n",
    "        #grab data that isn't a combination from the labels set\n",
    "        # create a set of size of 'batch size'\n",
    "        fam_mem_keys = list(family_member_map.keys())\n",
    "        while len(batch_family_members) < batch_size:\n",
    "            #get random numbers to choose\n",
    "            rand_num = np.random.randint(0, len(fam_mem_keys), 2)\n",
    "            #get random family member\n",
    "            fam_mem_1 = fam_mem_keys[rand_num[0]]\n",
    "            fam_mem_2 = fam_mem_keys[rand_num[1]]\n",
    "\n",
    "            #check if the two random people are family members - do not want that\n",
    "            condition_1 = (fam_mem_1, fam_mem_2)\n",
    "            condition_2 = (fam_mem_2, fam_mem_1)\n",
    "            if fam_mem_1 not in fam_mem_2 and condition_1 not in family_member_labels and condition_2 not in family_member_labels:\n",
    "                #add onto combination as not being family\n",
    "                batch_family_members.append(condition_1)\n",
    "                #add label as 0 for not family\n",
    "                label = np.concatenate([label,[0]])\n",
    "\n",
    "    #     print(batch_family_members)\n",
    "    #     print(label)\n",
    "    #     print('------------------')\n",
    "#         X1 = [choice(family_member_map[x[0]]) for x in batch_family_members]\n",
    "#         X1 = np.array([read_img(x) for x in X1])\n",
    "\n",
    "#         X2 = [choice(family_member_map[x[1]]) for x in batch_family_members]\n",
    "#         X2 = np.array([read_img(x) for x in X2])\n",
    "        \n",
    "        person_1_imgs = []\n",
    "        person_2_imgs = []\n",
    "        #get photos for each person\n",
    "        for fam_mem_tup in batch_family_members:\n",
    "            #peson 1's set of images\n",
    "            mem_1_all_imgs = family_member_map[fam_mem_tup[0]]\n",
    "            #person 2's set of images\n",
    "            mem_2_all_imgs = family_member_map[fam_mem_tup[1]]\n",
    "\n",
    "            #select random image\n",
    "            person_1_choice = choice(mem_1_all_imgs)\n",
    "            person_2_choice = choice(mem_2_all_imgs)\n",
    "            \n",
    "            #turn the images into arrays\n",
    "            for person, pic_path in enumerate([person_1_choice, person_2_choice]):\n",
    "                #read the image\n",
    "                img = cv2.imread(pic_path)\n",
    "                #turn to array\n",
    "                img = np.array(img).astype(np.float)\n",
    "                img_arr = preprocess_input(img)\n",
    "\n",
    "                #add to lst\n",
    "                if person == 0: #person 1\n",
    "                    person_1_imgs.append(img_arr)\n",
    "                else: #person 2\n",
    "                    person_2_imgs.append(img_arr)\n",
    "\n",
    "        #turn to numpy arrays\n",
    "        person_1_imgs = np.asarray(person_1_imgs)\n",
    "        person_2_imgs = np.asarray(person_2_imgs)\n",
    "#         print(person_1_imgs.shape)\n",
    "#         print(person_2_imgs.shape)\n",
    "    \n",
    "#         yield [X1, X2], label\n",
    "        yield [person_1_imgs, person_2_imgs], label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object gen at 0x1379869a8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(train, train_person_to_images_map, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "\n",
    "# Define Model Architecture \n",
    "def siamese_model(pretrained_model=None):\n",
    "    #left and right inputs for siamese network\n",
    "    left_image = layers.Input(shape = (224, 224, 3))\n",
    "    right_image = layers.Input(shape = (224, 224, 3))\n",
    "    \n",
    "    #start model\n",
    "    model = models.Sequential()\n",
    "    #check if there is a pretrained model\n",
    "    if pretrained_model is not None:\n",
    "        model.add(pretrained_model)\n",
    "        \n",
    "    model.add(layers.Dense(128, activation = \"relu\"))\n",
    "    pretrained_model.trainable = False\n",
    "    \n",
    "    x1 = model(left_image)\n",
    "    x2 = model(right_image)\n",
    "    \n",
    "    L2_normalized_layer_1 = layers.Lambda(lambda x: K.l2_normalize(x, axis = 1))\n",
    "    X1_normal = L2_normalized_layer_1(x1)\n",
    "    X2_normal = L2_normalized_layer_1(x2)\n",
    "\n",
    "    L1_layer = layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([X1_normal, X2_normal])\n",
    "    \n",
    "    prediction = layers.Dense(1, activation = \"sigmoid\")(L1_distance)\n",
    "    \n",
    "    siamese_net = models.Model(inputs = [left_image, right_image], outputs = prediction)\n",
    "\n",
    "    siamese_net.compile(loss = \"binary_crossentropy\", metrics = [\"acc\"], optimizer = optimizers.Adam(0.00001))\n",
    "    \n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (16, 1) was passed for an output of shape (None, 7, 7, 1) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-62cb78052be8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     validation_steps = 10)\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1236\u001b[0m     x, y, sample_weights = self._standardize_user_data(\n\u001b[1;32m   1237\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1238\u001b[0;31m         extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2655\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2657\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2658\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    510\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    511\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m                            \u001b[0;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                            'as the output.')\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (16, 1) was passed for an output of shape (None, 7, 7, 1) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "base_model = VGG19(weights='imagenet', include_top=False)\n",
    "kinship_model = siamese_model(pretrained_model=base_model)\n",
    "kinship_model.fit_generator(gen(train, train_person_to_images_map, batch_size = 16),\n",
    "                    validation_data = gen(val, val_person_to_images_map, batch_size = 16), \n",
    "                    epochs = 100, verbose = 2, \n",
    "                    steps_per_epoch = 200, \n",
    "                    validation_steps = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'kinship_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-7a07d824910d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m#predict using the test image arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkinship_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpic_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;31m#combine list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kinship_model' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_path = \"data/test/\"\n",
    "\n",
    "#return a set of inputerd size as generator\n",
    "def gen_2(test_set, size=32):\n",
    "    return (test_set[i:i + size] for i in range(0, len(test_set), size))\n",
    "\n",
    "submission_df = pd.read_csv('csv_files/sample_submission.csv')\n",
    "\n",
    "predictions = []\n",
    "\n",
    "\n",
    "for batch in tqdm(gen_2(submission_df.img_pair.values)):\n",
    "#     print(batch)\n",
    "    img_1 = []\n",
    "    img_2 = []\n",
    "    #seperate image paths\n",
    "    for pair_img in batch:\n",
    "        pairs = pair_img.split('-')\n",
    "        img_1.append(pairs[0])\n",
    "        img_2.append(pairs[1])\n",
    "    \n",
    "    pic_1 = []\n",
    "    pic_2 = []\n",
    "    #read the image names\n",
    "    for imge_1, imge_2 in zip(img_1, img_2):\n",
    "        pic_1.append(read_img(test_path + imge_1))\n",
    "        pic_2.append(read_img(test_path + imge_2))\n",
    "    \n",
    "    pic_1 = np.array(pic_1)\n",
    "    pic_2 = np.array(pic_2)\n",
    "#     print(pic_1)\n",
    "#     print(pic_2)\n",
    "    \n",
    "    #predict using the test image arrays \n",
    "    pred = kinship_model.predict([pic_1, pic_2]).ravel().tolist()\n",
    "    #combine list\n",
    "    predictions += pred\n",
    "    \n",
    "submission_df['is_related'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_pair</th>\n",
       "      <th>is_related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>face05508.jpg-face01210.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>face05750.jpg-face00898.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>face05820.jpg-face03938.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>face02104.jpg-face01172.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>face02428.jpg-face05611.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>face01219.jpg-face00274.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>face04262.jpg-face00555.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>face03697.jpg-face01892.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>face03524.jpg-face00319.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>face03410.jpg-face05368.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>face00292.jpg-face06004.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>face00353.jpg-face01203.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>face03140.jpg-face05223.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>face02915.jpg-face03312.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>face03012.jpg-face04103.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>face02240.jpg-face02336.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>face02131.jpg-face05209.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>face04105.jpg-face01209.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>face03565.jpg-face02509.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>face02836.jpg-face01540.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>face02832.jpg-face05386.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>face02596.jpg-face02913.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>face02231.jpg-face05835.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>face00451.jpg-face03664.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>face01644.jpg-face01682.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>face03988.jpg-face03379.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>face00908.jpg-face05944.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>face05117.jpg-face03498.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>face05466.jpg-face02942.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>face01537.jpg-face00187.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5280</th>\n",
       "      <td>face00411.jpg-face01806.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5281</th>\n",
       "      <td>face05891.jpg-face03366.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5282</th>\n",
       "      <td>face04767.jpg-face00956.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5283</th>\n",
       "      <td>face05678.jpg-face00743.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5284</th>\n",
       "      <td>face02271.jpg-face05413.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5285</th>\n",
       "      <td>face01206.jpg-face05478.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5286</th>\n",
       "      <td>face04927.jpg-face01139.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5287</th>\n",
       "      <td>face01529.jpg-face03186.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5288</th>\n",
       "      <td>face01982.jpg-face00593.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5289</th>\n",
       "      <td>face03028.jpg-face05083.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5290</th>\n",
       "      <td>face00217.jpg-face03266.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5291</th>\n",
       "      <td>face01124.jpg-face05416.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5292</th>\n",
       "      <td>face00550.jpg-face01305.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5293</th>\n",
       "      <td>face02955.jpg-face01873.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5294</th>\n",
       "      <td>face05274.jpg-face04345.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5295</th>\n",
       "      <td>face02024.jpg-face00384.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5296</th>\n",
       "      <td>face02310.jpg-face05600.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5297</th>\n",
       "      <td>face01912.jpg-face03882.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5298</th>\n",
       "      <td>face99999.jpg-face99996.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5299</th>\n",
       "      <td>face99999.jpg-face99995.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5300</th>\n",
       "      <td>face99999.jpg-face99994.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5301</th>\n",
       "      <td>face99999.jpg-face99993.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5302</th>\n",
       "      <td>face99998.jpg-face99996.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5303</th>\n",
       "      <td>face99998.jpg-face99995.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5304</th>\n",
       "      <td>face99998.jpg-face99994.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5305</th>\n",
       "      <td>face99998.jpg-face99993.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5306</th>\n",
       "      <td>face99997.jpg-face99996.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5307</th>\n",
       "      <td>face99997.jpg-face99995.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5308</th>\n",
       "      <td>face99997.jpg-face99994.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5309</th>\n",
       "      <td>face99997.jpg-face99993.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5310 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         img_pair  is_related\n",
       "0     face05508.jpg-face01210.jpg           0\n",
       "1     face05750.jpg-face00898.jpg           0\n",
       "2     face05820.jpg-face03938.jpg           0\n",
       "3     face02104.jpg-face01172.jpg           0\n",
       "4     face02428.jpg-face05611.jpg           0\n",
       "5     face01219.jpg-face00274.jpg           0\n",
       "6     face04262.jpg-face00555.jpg           0\n",
       "7     face03697.jpg-face01892.jpg           0\n",
       "8     face03524.jpg-face00319.jpg           0\n",
       "9     face03410.jpg-face05368.jpg           0\n",
       "10    face00292.jpg-face06004.jpg           0\n",
       "11    face00353.jpg-face01203.jpg           0\n",
       "12    face03140.jpg-face05223.jpg           0\n",
       "13    face02915.jpg-face03312.jpg           0\n",
       "14    face03012.jpg-face04103.jpg           0\n",
       "15    face02240.jpg-face02336.jpg           0\n",
       "16    face02131.jpg-face05209.jpg           0\n",
       "17    face04105.jpg-face01209.jpg           0\n",
       "18    face03565.jpg-face02509.jpg           0\n",
       "19    face02836.jpg-face01540.jpg           0\n",
       "20    face02832.jpg-face05386.jpg           0\n",
       "21    face02596.jpg-face02913.jpg           0\n",
       "22    face02231.jpg-face05835.jpg           0\n",
       "23    face00451.jpg-face03664.jpg           0\n",
       "24    face01644.jpg-face01682.jpg           0\n",
       "25    face03988.jpg-face03379.jpg           0\n",
       "26    face00908.jpg-face05944.jpg           0\n",
       "27    face05117.jpg-face03498.jpg           0\n",
       "28    face05466.jpg-face02942.jpg           0\n",
       "29    face01537.jpg-face00187.jpg           0\n",
       "...                           ...         ...\n",
       "5280  face00411.jpg-face01806.jpg           0\n",
       "5281  face05891.jpg-face03366.jpg           0\n",
       "5282  face04767.jpg-face00956.jpg           0\n",
       "5283  face05678.jpg-face00743.jpg           0\n",
       "5284  face02271.jpg-face05413.jpg           0\n",
       "5285  face01206.jpg-face05478.jpg           0\n",
       "5286  face04927.jpg-face01139.jpg           0\n",
       "5287  face01529.jpg-face03186.jpg           0\n",
       "5288  face01982.jpg-face00593.jpg           0\n",
       "5289  face03028.jpg-face05083.jpg           0\n",
       "5290  face00217.jpg-face03266.jpg           0\n",
       "5291  face01124.jpg-face05416.jpg           0\n",
       "5292  face00550.jpg-face01305.jpg           0\n",
       "5293  face02955.jpg-face01873.jpg           0\n",
       "5294  face05274.jpg-face04345.jpg           0\n",
       "5295  face02024.jpg-face00384.jpg           0\n",
       "5296  face02310.jpg-face05600.jpg           0\n",
       "5297  face01912.jpg-face03882.jpg           0\n",
       "5298  face99999.jpg-face99996.jpg           0\n",
       "5299  face99999.jpg-face99995.jpg           0\n",
       "5300  face99999.jpg-face99994.jpg           0\n",
       "5301  face99999.jpg-face99993.jpg           0\n",
       "5302  face99998.jpg-face99996.jpg           0\n",
       "5303  face99998.jpg-face99995.jpg           0\n",
       "5304  face99998.jpg-face99994.jpg           0\n",
       "5305  face99998.jpg-face99993.jpg           0\n",
       "5306  face99997.jpg-face99996.jpg           0\n",
       "5307  face99997.jpg-face99995.jpg           0\n",
       "5308  face99997.jpg-face99994.jpg           0\n",
       "5309  face99997.jpg-face99993.jpg           0\n",
       "\n",
       "[5310 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# test_path = \"data/test/\"\n",
    "\n",
    "# def chunker(seq, size=32):\n",
    "#     return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "# submission = pd.read_csv('csv_files/sample_submission.csv')\n",
    "\n",
    "# predictions = []\n",
    "\n",
    "# chunker(submission.img_pair.values)\n",
    "\n",
    "# for batch in tqdm(chunker(submission.img_pair.values)):\n",
    "# #     print(batch)\n",
    "    \n",
    "    \n",
    "#     X1 = [x.split(\"-\")[0] for x in batch]\n",
    "#     X1 = np.array([read_img(test_path + x) for x in X1])\n",
    "\n",
    "#     X2 = [x.split(\"-\")[1] for x in batch]\n",
    "#     X2 = np.array([read_img(test_path + x) for x in X2])\n",
    "\n",
    "#     print(X1)\n",
    "# #     print(X2)\n",
    "    \n",
    "    \n",
    "# #     pred = kinship_model.predict([X1, X2]).ravel().tolist()\n",
    "# #     predictions += pred\n",
    "\n",
    "# # submission['is_related'] = predictions\n",
    "\n",
    "# # submission.to_csv(\"vgg_face.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kinship_model_json = kinship_model.to_json()\n",
    "# with open(\"kinship_model.json\", \"w\") as json_file:\n",
    "#     json_file.write(kinship_model_json)\n",
    "    \n",
    "# kinship_model.save_weights(\"kinship_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "# def baseline_model():\n",
    "#     input_1 = Input(shape=(224, 224, 3))\n",
    "#     input_2 = Input(shape=(224, 224, 3))\n",
    "\n",
    "# #     base_model = VGGFace(model='resnet50', include_top=False)\n",
    "    \n",
    "#     base_model = VGG19(weights='imagenet', include_top=False)\n",
    "    \n",
    "#     for x in base_model.layers[:-3]:\n",
    "#         x.trainable = True\n",
    "\n",
    "#     x1 = base_model(input_1)\n",
    "#     x2 = base_model(input_2)\n",
    "\n",
    "#     # x1_ = Reshape(target_shape=(7*7, 2048))(x1)\n",
    "#     # x2_ = Reshape(target_shape=(7*7, 2048))(x2)\n",
    "#     #\n",
    "#     # x_dot = Dot(axes=[2, 2], normalize=True)([x1_, x2_])\n",
    "#     # x_dot = Flatten()(x_dot)\n",
    "\n",
    "#     x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n",
    "#     x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n",
    "\n",
    "#     x3 = Subtract()([x1, x2])\n",
    "#     x3 = Multiply()([x3, x3])\n",
    "\n",
    "#     x = Multiply()([x1, x2])\n",
    "\n",
    "#     x = Concatenate(axis=-1)([x, x3])\n",
    "\n",
    "#     x = Dense(100, activation=\"relu\")(x)\n",
    "#     x = Dropout(0.01)(x)\n",
    "#     out = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "#     model = Model([input_1, input_2], out)\n",
    "\n",
    "#     model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n",
    "\n",
    "# #     model.summary()\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = baseline_model()\n",
    "# # model.load_weights(file_path)\n",
    "# model.fit_generator(gen(train, train_person_to_images_map, batch_size=16), use_multiprocessing=True,\n",
    "#                     validation_data=gen(val, val_person_to_images_map, batch_size=16), epochs=20, verbose=2,\n",
    "#                     workers=4, steps_per_epoch=200, validation_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object gen at 0x13db494f8>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def read_img(path):\n",
    "#     img = cv2.imread(path)\n",
    "#     img = np.array(img).astype(np.float)\n",
    "#     return preprocess_input(img)\n",
    "\n",
    "\n",
    "# def gen(list_tuples, person_to_images_map, batch_size=16):\n",
    "#     ppl = list(person_to_images_map.keys())\n",
    "#     while True:\n",
    "#         batch_tuples = sample(list_tuples, batch_size // 2)\n",
    "#         labels = [1] * len(batch_tuples)\n",
    "# #         print(batch_tuples)\n",
    "# #         print(labels)\n",
    "#         while len(batch_tuples) < batch_size:\n",
    "#             p1 = choice(ppl)\n",
    "#             p2 = choice(ppl)\n",
    "# #             print('p1: ', p1)\n",
    "# #             print('p2: ', p2)\n",
    "#             if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n",
    "#                 batch_tuples.append((p1, p2))\n",
    "#                 labels.append(0)\n",
    "# #         print(batch_tuples)\n",
    "# #         print(labels)\n",
    "#         for x in batch_tuples:\n",
    "# #             print('x:, ', x)\n",
    "#             if not len(person_to_images_map[x[0]]):\n",
    "# #                 print('x[0]:, ', x)\n",
    "#                 print(x[0])\n",
    "# #         print('------------------------')\n",
    "#         X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n",
    "# #         print(len(X1))\n",
    "# #         print(X1)\n",
    "# #         print('------------------------')\n",
    "#         X1 = np.array([read_img(x) for x in X1])\n",
    "# #         print(X1)\n",
    "# #         print('------------------------')\n",
    "#         X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n",
    "#         X2 = np.array([read_img(x) for x in X2])\n",
    "        \n",
    "# #         print(type(X1))\n",
    "# #         print(type(X2))\n",
    "# #         print([X1,X2])\n",
    "#         yield [X1, X2], labels\n",
    "\n",
    "\n",
    "# gen(train, train_person_to_images_map, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen(train, train_person_to_images_map, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen(list_tuples, person_to_images_map, batch_size=16):\n",
    "#     ppl = list(person_to_images_map.keys())\n",
    "#     while True:\n",
    "#         batch_tuples = sample(list_tuples, batch_size // 2)\n",
    "#         labels = [1] * len(batch_tuples)\n",
    "#         while len(batch_tuples) < batch_size:\n",
    "#             p1 = choice(ppl)\n",
    "#             p2 = choice(ppl)\n",
    "\n",
    "#             if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n",
    "#                 batch_tuples.append((p1, p2))\n",
    "#                 labels.append(0)\n",
    "\n",
    "#         for x in batch_tuples:\n",
    "#             if not len(person_to_images_map[x[0]]):\n",
    "#                 print(x[0])\n",
    "\n",
    "#         X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n",
    "#         X1 = np.array([read_img(x) for x in X1])\n",
    "\n",
    "#         X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n",
    "#         X2 = np.array([read_img(x) for x in X2])\n",
    "        \n",
    "# #         print(X1)\n",
    "        \n",
    "#         yield [X1, X2], labels\n",
    "        \n",
    "# # gen(train, train_person_to_images_map, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install opencv-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
