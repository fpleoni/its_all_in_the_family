{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.math import sqrt, square, reduce_sum\n",
    "from random import choice, sample\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "train_file_path = \"../data/train/\"\n",
    "train_relationships_path = \"../csv_files/train_relationships.csv\"\n",
    "validation_path = \"F09\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions by Youness Mansar https://github.com/CVxTz/kinship_prediction\n",
    "all_images = glob(train_file_path + \"*/*/*.jpg\")\n",
    "\n",
    "train_images = [x for x in all_images if validation_path not in x]\n",
    "val_images = [x for x in all_images if validation_path in x]\n",
    "\n",
    "train_person_to_images_map = defaultdict(list)\n",
    "\n",
    "ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\n",
    "\n",
    "for x in train_images:\n",
    "    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "\n",
    "val_person_to_images_map = defaultdict(list)\n",
    "\n",
    "for x in val_images:\n",
    "    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "\n",
    "relationships = pd.read_csv(train_relationships_path)\n",
    "relationships = list(zip(relationships.p1.values, relationships.p2.values))\n",
    "relationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n",
    "\n",
    "train = [x for x in relationships if validation_path not in x[0]]\n",
    "val = [x for x in relationships if validation_path in x[0]]\n",
    "\n",
    "\n",
    "def read_img(path):\n",
    "    img = preprocessing.image.load_img(path, target_size=(160, 160))\n",
    "    img = preprocessing.image.img_to_array(img)\n",
    "    img = preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def gen(list_tuples, person_to_images_map, batch_size=16):\n",
    "    ppl = list(person_to_images_map.keys())\n",
    "    while True:\n",
    "        batch_tuples = sample(list_tuples, batch_size // 2)\n",
    "        labels = [1] * len(batch_tuples)\n",
    "        while len(batch_tuples) < batch_size:\n",
    "            p1 = choice(ppl)\n",
    "            p2 = choice(ppl)\n",
    "\n",
    "            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n",
    "                batch_tuples.append((p1, p2))\n",
    "                labels.append(0)\n",
    "\n",
    "        for x in batch_tuples:\n",
    "            if not len(person_to_images_map[x[0]]):\n",
    "                print(x[0])\n",
    "\n",
    "        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n",
    "        X1 = np.array([read_img(x) for x in X1])\n",
    "\n",
    "        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n",
    "        X2 = np.array([read_img(x) for x in X2])\n",
    "\n",
    "        yield [X1, X2], labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    # Contrastive Loss from Hadsell-et-al. 2006.\n",
    "    # http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_pred) * K.square(K.maximum(margin - y_pred, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Import FaceNet model and FaceNet weights\n",
    "facenet_model = models.load_model(\"../facenet/facenet_keras.h5\")\n",
    "facenet_model.load_weights(\"../facenet/facenet_keras_weights.h5\")\n",
    " \n",
    "facenet_model.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in facenet_model.layers:\n",
    "    if layer.name in [\"Mixed_7a_Branch_2_Conv2d_0a_1x1\", \"Block8_1_Branch_1_Conv2d_0a_1x1\"]:\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "# layers = [(layer, layer.name, layer.trainable) for layer in facenet_model.layers]\n",
    "# pd.set_option(\"display.max_rows\", 500)\n",
    "# pd.DataFrame(layers, columns=[\"Layer Type\", \"Layer Name\", \"Layer Trainable\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Architecture \n",
    "def siamese_model():\n",
    "    left_image = layers.Input(shape = (160, 160, 3))\n",
    "    right_image = layers.Input(shape = (160, 160, 3))\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(facenet_model)\n",
    "    model.add(layers.Dense(128, activation = \"tanh\"))\n",
    "    \n",
    "    x1 = model(left_image)\n",
    "    x2 = model(right_image)\n",
    "    \n",
    "    L2_normalized_layer_1 = layers.Lambda(lambda x: K.l2_normalize(x, axis = 1))\n",
    "    X1_normal = L2_normalized_layer_1(x1)\n",
    "    X2_normal = L2_normalized_layer_1(x2)\n",
    "\n",
    "    L1_layer = layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([X1_normal, X2_normal])\n",
    "\n",
    "    prediction = layers.Dense(1, activation = \"sigmoid\")(L1_distance)\n",
    "    \n",
    "    siamese_net = models.Model(inputs = [left_image, right_image], outputs = prediction)\n",
    "\n",
    "    siamese_net.compile(loss = contrastive_loss, metrics = [\"acc\"], optimizer = optimizers.Adam(0.0001))\n",
    "    \n",
    "    return siamese_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor = \"val_acc\", mode = \"max\", patience = 20)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\"best_kinship_facenet_model_2.h5\", monitor = \"val_acc\", \n",
    "                                             mode = \"max\", save_best_only = True)\n",
    "\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(monitor = \"val_acc\", mode = \"max\", patience = 10, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'Input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4c2cfcb02bdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkinship_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msiamese_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m history = kinship_model.fit_generator(gen(train, train_person_to_images_map, batch_size = 100),\n\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_person_to_images_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             callbacks = [early_stop, model_checkpoint, reduce_lr_on_plateau])\n",
      "\u001b[0;32m<ipython-input-7-4f49daf698a1>\u001b[0m in \u001b[0;36msiamese_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define Model Architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msiamese_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mleft_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mright_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'Input'"
     ]
    }
   ],
   "source": [
    "kinship_model = siamese_model()\n",
    "history = kinship_model.fit_generator(gen(train, train_person_to_images_map, batch_size = 100),\n",
    "                    validation_data = gen(val, val_person_to_images_map, batch_size = 100), epochs = 200, verbose = 2,\n",
    "                steps_per_epoch = 200, validation_steps = 100, \n",
    "                            callbacks = [early_stop, model_checkpoint, reduce_lr_on_plateau])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinship_model_json = kinship_model.to_json()\n",
    "with open(\"kinship_model_facenet_2.json\", \"w\") as json_file:\n",
    "    json_file.write(kinship_model_json)\n",
    "    \n",
    "kinship_model.save_weights(\"kinship_model_weights_facenet_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history[\"acc\"], color = \"aquamarine\")\n",
    "plt.plot(history.history[\"val_acc\"], color = \"magenta\")\n",
    "plt.title(\"Model accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"validation\"], loc = \"upper left\")\n",
    "plt.show()\n",
    "plt.savefig(\"facenet_2_acc.png\")\n",
    "# summarize history for loss\n",
    "plt.plot(history.history[\"loss\"], \"aquamarine\")\n",
    "plt.plot(history.history[\"val_loss\"], color = \"magenta\")\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"validation\"], loc = \"upper left\")\n",
    "plt.show()\n",
    "plt.savefig(\"facenet_2_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"../data/test/\"\n",
    "\n",
    "#return a set of inputerd size as generator\n",
    "def gen_2(test_set, size=32):\n",
    "    return (test_set[i:i + size] for i in range(0, len(test_set), size))\n",
    "\n",
    "submission_df = pd.read_csv(\"../csv_files/sample_submission.csv\")\n",
    "\n",
    "predictions = []\n",
    "\n",
    "\n",
    "for batch in gen_2(submission_df.img_pair.values):\n",
    "#     print(batch)\n",
    "    img_1 = []\n",
    "    img_2 = []\n",
    "    #seperate image paths\n",
    "    for pair_img in batch:\n",
    "        pairs = pair_img.split('-')\n",
    "        img_1.append(pairs[0])\n",
    "        img_2.append(pairs[1])\n",
    "    \n",
    "    pic_1 = []\n",
    "    pic_2 = []\n",
    "    #read the image names\n",
    "    for imge_1, imge_2 in zip(img_1, img_2):\n",
    "        pic_1.append(read_img(test_path + imge_1))\n",
    "        pic_2.append(read_img(test_path + imge_2))\n",
    "    \n",
    "    pic_1 = np.array(pic_1)\n",
    "    pic_2 = np.array(pic_2)\n",
    "#     print(pic_1)\n",
    "#     print(pic_2)\n",
    "    \n",
    "    #predict using the test image arrays \n",
    "    pred = kinship_model.predict([pic_1, pic_2]).ravel().tolist()\n",
    "    #combine list\n",
    "    predictions += pred\n",
    "    \n",
    "submission_df[\"predicted_relationship\"] = predictions\n",
    "submission_df.to_csv(\"facenet_2_predictions.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
