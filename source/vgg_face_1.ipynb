{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/florencialeoni/anaconda3/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from random import choice, sample\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "train_file_path = \"../data/train/\"\n",
    "train_relationships_path = \"../csv_files/train_relationships.csv\"\n",
    "validation_path = \"F09\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions by Youness Mansar https://github.com/CVxTz/kinship_prediction\n",
    "all_images = glob(train_file_path + \"*/*/*.jpg\")\n",
    "\n",
    "train_images = [x for x in all_images if validation_path not in x]\n",
    "val_images = [x for x in all_images if validation_path in x]\n",
    "\n",
    "train_person_to_images_map = defaultdict(list)\n",
    "\n",
    "ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\n",
    "\n",
    "for x in train_images:\n",
    "    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "\n",
    "val_person_to_images_map = defaultdict(list)\n",
    "\n",
    "for x in val_images:\n",
    "    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "\n",
    "relationships = pd.read_csv(train_relationships_path)\n",
    "relationships = list(zip(relationships.p1.values, relationships.p2.values))\n",
    "relationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n",
    "\n",
    "train = [x for x in relationships if validation_path not in x[0]]\n",
    "val = [x for x in relationships if validation_path in x[0]]\n",
    "\n",
    "\n",
    "def read_img(path):\n",
    "    img = preprocessing.image.load_img(path, target_size=(224, 224))\n",
    "    img = preprocessing.image.img_to_array(img)\n",
    "#     img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def gen(list_tuples, person_to_images_map, batch_size=16):\n",
    "    ppl = list(person_to_images_map.keys())\n",
    "    while True:\n",
    "        batch_tuples = sample(list_tuples, batch_size // 2)\n",
    "        labels = [1] * len(batch_tuples)\n",
    "        while len(batch_tuples) < batch_size:\n",
    "            p1 = choice(ppl)\n",
    "            p2 = choice(ppl)\n",
    "\n",
    "            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n",
    "                batch_tuples.append((p1, p2))\n",
    "                labels.append(0)\n",
    "\n",
    "        for x in batch_tuples:\n",
    "            if not len(person_to_images_map[x[0]]):\n",
    "                print(x[0])\n",
    "\n",
    "        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n",
    "        X1 = np.array([read_img(x) for x in X1])\n",
    "\n",
    "        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n",
    "        X2 = np.array([read_img(x) for x in X2])\n",
    "\n",
    "        yield [X1, X2], labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG Face Implementation by Jia Zhen \n",
    "# https://gist.github.com/jz3707/0588bc401f1150b556d9ceb01668e790\n",
    "\n",
    "def vggface_model(weight_path):\n",
    "    \n",
    "    image = layers.Input(shape = (224, 224, 3))\n",
    "    \n",
    "    # VGG FACE\n",
    "    pad_1 = layers.ZeroPadding2D((1,1),input_shape = (224,224, 3))(image)\n",
    "    conv_1 = layers.Convolution2D(64, (3, 3), activation = \"relu\")(pad_1)\n",
    "    pad_1_2 = layers.ZeroPadding2D((1,1))(conv_1)\n",
    "    conv_1_2 = layers.Convolution2D(64, (3, 3), activation = \"relu\")(pad_1_2)\n",
    "    pool_1 = layers.MaxPooling2D((2,2), strides = (2,2))(conv_1_2)\n",
    "\n",
    "    pad_2 = layers.ZeroPadding2D((1,1))(pool_1)\n",
    "    conv_2 = layers.Convolution2D(128, (3, 3), activation = \"relu\")(pad_2)\n",
    "    pad_2_2 = layers.ZeroPadding2D((1,1))(conv_2)\n",
    "    conv_2_2 = layers.Convolution2D(128, (3, 3), activation = \"relu\")(pad_2_2)\n",
    "    pool_2 = layers.MaxPooling2D((2,2), strides = (2,2))(conv_2_2)\n",
    "\n",
    "    pad_3 = layers.ZeroPadding2D((1,1))(pool_2)\n",
    "    conv_3 = layers.Convolution2D(256, (3, 3), activation = \"relu\")(pad_3)\n",
    "    pad_3_2 = layers.ZeroPadding2D((1,1))(conv_3)\n",
    "    conv_3_2 = layers.Convolution2D(256, (3, 3), activation = \"relu\")(pad_3_2)\n",
    "    pad_3_3 = layers.ZeroPadding2D((1,1))(conv_3_2)\n",
    "    conv_3_3 = layers.Convolution2D(256, (3, 3), activation = \"relu\")(pad_3_3)\n",
    "    pool_3 = layers.MaxPooling2D((2,2), strides = (2,2))(conv_3_3)\n",
    "\n",
    "    pad_4 = layers.ZeroPadding2D((1,1))(pool_3)\n",
    "    conv_4 = layers.Convolution2D(512, (3, 3), activation = \"relu\")(pad_4)\n",
    "    pad_4_2 = layers.ZeroPadding2D((1,1))(conv_4)\n",
    "    conv_4_2 = layers.Convolution2D(512, (3, 3), activation = \"relu\")(pad_4_2)\n",
    "    pad_4_3 = layers.ZeroPadding2D((1,1))(conv_4_2)\n",
    "    conv_4_3 = layers.Convolution2D(512, (3, 3), activation = \"relu\")(pad_4_3)\n",
    "    pool_4 = layers.MaxPooling2D((2,2), strides = (2,2))(conv_4_3)\n",
    "\n",
    "    pad_5 = layers.ZeroPadding2D((1,1))(pool_4)\n",
    "    conv_5 = layers.Convolution2D(512, (3, 3), activation = \"relu\")(pad_5)\n",
    "    pad_5_2 = layers.ZeroPadding2D((1,1))(conv_5)\n",
    "    conv_5_2 = layers.Convolution2D(512, (3, 3), activation = \"relu\")(pad_5_2)\n",
    "    pad_5_3 = layers.ZeroPadding2D((1,1))(conv_5_2)\n",
    "    conv_5_3 = layers.Convolution2D(512, (3, 3), activation = \"relu\")(pad_5_3)\n",
    "    pool_5 = layers.MaxPooling2D((2,2), strides = (2,2))(conv_5_3)\n",
    "\n",
    "    conv_6 = layers.Convolution2D(4096, (7, 7), activation = \"relu\")(pool_5)\n",
    "    drop_6 = layers.Dropout(0.5)(conv_6)\n",
    "    conv_6_1 = layers.Convolution2D(4096, (1, 1), activation = \"relu\")(drop_6)\n",
    "    drop_6_1 = layers.Dropout(0.5)(conv_6_1)\n",
    "    conv_6_2 = layers.Convolution2D(2622, (1, 1))(drop_6_1)\n",
    "    flat_6 = layers.Flatten()(conv_6_2)\n",
    "    out = layers.Activation(\"softmax\")(flat_6)\n",
    "    \n",
    "    vgg_face_model = models.Model(inputs = image, outputs = out)\n",
    "    \n",
    "    # VGG FACE WEIGHTS\n",
    "    vgg_face_model.load_weights(weight_path)\n",
    "    \n",
    "    vgg_face_model.trainable = False\n",
    "    \n",
    "    return vgg_face_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Architecture \n",
    "def siamese_model():\n",
    "    left_image = layers.Input(shape = (224, 224, 3))\n",
    "    right_image = layers.Input(shape = (224, 224, 3))\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(vggface_model(\"../vggface/vgg_face_weights.h5\"))\n",
    "    model.add(layers.Dense(128, activation = \"relu\"))\n",
    "    \n",
    "    x1 = model(left_image)\n",
    "    x2 = model(right_image)\n",
    "    \n",
    "    L2_normalized_layer_1 = layers.Lambda(lambda x: K.l2_normalize(x, axis = 1))\n",
    "    X1_normal = L2_normalized_layer_1(x1)\n",
    "    X2_normal = L2_normalized_layer_1(x2)\n",
    "\n",
    "    L1_layer = layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([X1_normal, X2_normal])\n",
    "    \n",
    "    prediction = layers.Dense(1, activation = \"sigmoid\")(L1_distance)\n",
    "    \n",
    "    siamese_net = models.Model(inputs = [left_image, right_image], outputs = prediction)\n",
    "\n",
    "    siamese_net.compile(loss = \"binary_crossentropy\", metrics = [\"acc\"], optimizer = optimizers.Adam(0.00001))\n",
    "    \n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e702caebf79a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m kinship_model.fit_generator(gen(train, train_person_to_images_map, batch_size = 100),\n\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_person_to_images_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                 steps_per_epoch = 200, validation_steps = 100)                         \n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1189\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kinship_model = siamese_model()\n",
    "kinship_model.fit_generator(gen(train, train_person_to_images_map, batch_size = 100),\n",
    "                    validation_data = gen(val, val_person_to_images_map, batch_size = 100), epochs = 20, verbose = 2,\n",
    "                steps_per_epoch = 200, validation_steps = 100)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinship_model_json = kinship_model.to_json()\n",
    "with open(\"kinship_model_vgg.json\", \"w\") as json_file:\n",
    "    json_file.write(kinship_model_json)\n",
    "    \n",
    "kinship_model.save_weights(\"kinship_model_weights_vgg.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
