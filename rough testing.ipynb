{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from random import choice, sample\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, Lambda, Dense, Flatten,MaxPooling2D,Activation, Dropout, BatchNormalization,  GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Subtract,Multiply\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from skimage.io import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from random import sample, choice\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "train_file_path = \"data/train/\"\n",
    "train_relationships_path = \"../csv_files/train_relationships.csv\"\n",
    "#the validation set will be family members F09...\n",
    "validation_set = \"F09\"\n",
    "\n",
    "#get all the images\n",
    "all_images = glob(train_file_path + \"*/*/*.jpg\")\n",
    "\n",
    "#seperate the train and validation sets\n",
    "train_images = [img for img in all_images if validation_set not in img]\n",
    "val_images = [img for img in all_images if validation_set in img]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12379\n"
     ]
    }
   ],
   "source": [
    "print(len(all_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_person_to_images_map = defaultdict(list)\n",
    "\n",
    "# ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\n",
    "\n",
    "# for x in train_images:\n",
    "#     train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "\n",
    "# val_person_to_images_map = defaultdict(list)\n",
    "\n",
    "# for x in val_images:\n",
    "#     val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary with key=family member and value=list of pictures of family member\n",
    "def fam_mem_pics(all_images):\n",
    "    #create dictionary\n",
    "    fam_member_dict = {}\n",
    "    #create list for member pictures\n",
    "    fam_member_lst = []\n",
    "    i = 0\n",
    "    while i < len(all_images)-1:\n",
    "        #get the family member as key\n",
    "        split_path = all_images[i].split('/')\n",
    "        key = split_path[2]+'/'+split_path[3]\n",
    "        #check if picture is about the same fmaily member\n",
    "        same_member = True\n",
    "\n",
    "        while same_member:\n",
    "            #check if same family member\n",
    "            if key in all_images[i]:\n",
    "                #append the image to the family member list\n",
    "                fam_member_lst.append(all_images[i])\n",
    "                \n",
    "            else:\n",
    "                #changed family member\n",
    "                same_member = False\n",
    "            \n",
    "            #update the counter to not go out of bounds\n",
    "            if (i+1) < len(all_images):  \n",
    "                i += 1\n",
    "            else:\n",
    "                #break if out of bounds\n",
    "                break\n",
    "                \n",
    "        #check if key is already in dictionary - just in case the images are out of order\n",
    "        if key in fam_member_dict.keys():\n",
    "            #combine list\n",
    "            fam_member_dict[key] = fam_member_dict[key] + fam_member_lst\n",
    "        else: \n",
    "            #create key=family member and value= list of family member's pictures\n",
    "            fam_member_dict[key] = fam_member_lst\n",
    "        \n",
    "        fam_member_lst = []\n",
    "        \n",
    "    #return the member_dictionary \n",
    "    return fam_member_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a family member image dictionary\n",
    "train_person_to_images_map = fam_mem_pics(train_images)\n",
    "val_person_to_images_map = fam_mem_pics(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_person_to_images_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_person_to_images_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the raltionships dataframe\n",
    "df = pd.read_csv(\"csv_files/train_relationships.csv\")\n",
    "\n",
    "#get isolate the the family/member portions to compare\n",
    "all_family_members = []\n",
    "for img in all_images:\n",
    "    split_path = img.split('/')\n",
    "    all_family_members.append(split_path[2]+'/'+split_path[3])\n",
    "\n",
    "#some relationships are not present within the relationship csv\n",
    "#remove them by only keeping the ones mentioned in the dataset\n",
    "df = df[df['p1'].isin(all_family_members)]\n",
    "df = df[df['p2'].isin(all_family_members)]\n",
    "\n",
    "#seperate the training and validation labels\n",
    "train = df[~df['p1'].str.contains(validation_set)]\n",
    "val = df[df['p1'].str.contains(validation_set)]\n",
    "\n",
    "#only keep the values in the dictionary keys\n",
    "train = train[train['p1'].isin(list(train_person_to_images_map.keys()))]\n",
    "val = val[val['p1'].isin(list(val_person_to_images_map.keys()))]\n",
    "\n",
    "train = train[train['p2'].isin(list(train_person_to_images_map.keys()))]\n",
    "val = val[val['p2'].isin(list(val_person_to_images_map.keys()))]\n",
    "\n",
    "#turn dataframes to tuples to get labels\n",
    "train = list(zip(train['p1'].values, train['p2'].values))\n",
    "val = list(zip(val['p1'].values, val['p2'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('F0002/MID1', 'F0002/MID3'),\n",
       " ('F0002/MID2', 'F0002/MID3'),\n",
       " ('F0005/MID1', 'F0005/MID2'),\n",
       " ('F0005/MID3', 'F0005/MID2'),\n",
       " ('F0009/MID1', 'F0009/MID4'),\n",
       " ('F0009/MID1', 'F0009/MID3'),\n",
       " ('F0009/MID1', 'F0009/MID2'),\n",
       " ('F0009/MID1', 'F0009/MID6'),\n",
       " ('F0009/MID2', 'F0009/MID4'),\n",
       " ('F0009/MID2', 'F0009/MID6')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train))\n",
    "train[:10] #labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_img(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = np.array(img).astype(np.float)\n",
    "    return preprocess_input(img)\n",
    "\n",
    "#create a generator\n",
    "def gen(family_member_labels, family_member_map, batch_size=16):\n",
    "    while True:\n",
    "        #sample from half of true labels - don't repeat grab\n",
    "        half_batch_size = int(batch_size/2)\n",
    "        batch_family_members = sample(family_member_labels, half_batch_size)\n",
    "        #all these should be of label 1\n",
    "        label = np.ones(len(batch_family_members))\n",
    "\n",
    "        #grab data that isn't a combination from the labels set\n",
    "        # create a set of size of 'batch size'\n",
    "        fam_mem_keys = list(family_member_map.keys())\n",
    "        while len(batch_family_members) < batch_size:\n",
    "            #get random numbers to choose\n",
    "            rand_num = np.random.randint(0, len(fam_mem_keys), 2)\n",
    "            #get random family member\n",
    "            fam_mem_1 = fam_mem_keys[rand_num[0]]\n",
    "            fam_mem_2 = fam_mem_keys[rand_num[1]]\n",
    "\n",
    "            #check if the two random people are family members - do not want that\n",
    "            condition_1 = (fam_mem_1, fam_mem_2)\n",
    "            condition_2 = (fam_mem_2, fam_mem_1)\n",
    "            if fam_mem_1 not in fam_mem_2 and condition_1 not in family_member_labels and condition_2 not in family_member_labels:\n",
    "                #add onto combination as not being family\n",
    "                batch_family_members.append(condition_1)\n",
    "                #add label as 0 for not family\n",
    "                label = np.concatenate([label,[0]])\n",
    "\n",
    "    #     print(batch_family_members)\n",
    "    #     print(label)\n",
    "    #     print('------------------')\n",
    "#         X1 = [choice(family_member_map[x[0]]) for x in batch_family_members]\n",
    "#         X1 = np.array([read_img(x) for x in X1])\n",
    "\n",
    "#         X2 = [choice(family_member_map[x[1]]) for x in batch_family_members]\n",
    "#         X2 = np.array([read_img(x) for x in X2])\n",
    "        \n",
    "        person_1_imgs = []\n",
    "        person_2_imgs = []\n",
    "        #get photos for each person\n",
    "        for fam_mem_tup in batch_family_members:\n",
    "            #peson 1's set of images\n",
    "            mem_1_all_imgs = family_member_map[fam_mem_tup[0]]\n",
    "            #person 2's set of images\n",
    "            mem_2_all_imgs = family_member_map[fam_mem_tup[1]]\n",
    "\n",
    "            #select random image\n",
    "            person_1_choice = choice(mem_1_all_imgs)\n",
    "            person_2_choice = choice(mem_2_all_imgs)\n",
    "            \n",
    "            #turn the images into arrays\n",
    "            for person, pic_path in enumerate([person_1_choice, person_2_choice]):\n",
    "                #read the image\n",
    "                img = cv2.imread(pic_path)\n",
    "                #turn to array\n",
    "                img = np.array(img).astype(np.float)\n",
    "                img_arr = preprocess_input(img)\n",
    "\n",
    "                #add to lst\n",
    "                if person == 0: #person 1\n",
    "                    person_1_imgs.append(img_arr)\n",
    "                else: #person 2\n",
    "                    person_2_imgs.append(img_arr)\n",
    "\n",
    "        #turn to numpy arrays\n",
    "        person_1_imgs = np.asarray(person_1_imgs)\n",
    "        person_2_imgs = np.asarray(person_2_imgs)\n",
    "#         print(person_1_imgs.shape)\n",
    "#         print(person_2_imgs.shape)\n",
    "    \n",
    "#         yield [X1, X2], label\n",
    "        yield [person_1_imgs, person_2_imgs], label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object gen at 0x146759cf0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(train, train_person_to_images_map, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "\n",
    "# Define Model Architecture \n",
    "def siamese_model(pretrained_model=None):\n",
    "    #left and right inputs for siamese network\n",
    "    left_image = layers.Input(shape = (224, 224, 3))\n",
    "    right_image = layers.Input(shape = (224, 224, 3))\n",
    "    \n",
    "    #start model\n",
    "    model = models.Sequential()\n",
    "    #check if there is a pretrained model\n",
    "    if pretrained_model is not None:\n",
    "        model.add(pretrained_model)\n",
    "        \n",
    "    model.add(layers.Dense(128, activation = \"relu\"))\n",
    "    pretrained_model.trainable = False\n",
    "    \n",
    "    x1 = model(left_image)\n",
    "    x2 = model(right_image)\n",
    "    \n",
    "    L2_normalized_layer_1 = layers.Lambda(lambda x: K.l2_normalize(x, axis = 1))\n",
    "    X1_normal = L2_normalized_layer_1(x1)\n",
    "    X2_normal = L2_normalized_layer_1(x2)\n",
    "\n",
    "    L1_layer = layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([X1_normal, X2_normal])\n",
    "    \n",
    "    prediction = layers.Dense(1, activation = \"sigmoid\")(L1_distance)\n",
    "    \n",
    "    siamese_net = models.Model(inputs = [left_image, right_image], outputs = prediction)\n",
    "\n",
    "    siamese_net.compile(loss = \"binary_crossentropy\", metrics = [\"acc\"], optimizer = optimizers.Adam(0.00001))\n",
    "    \n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (16, 1) was passed for an output of shape (None, 7, 7, 1) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-62cb78052be8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     validation_steps = 10)\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1236\u001b[0m     x, y, sample_weights = self._standardize_user_data(\n\u001b[1;32m   1237\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1238\u001b[0;31m         extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2655\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2657\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2658\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    510\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    511\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m                            \u001b[0;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                            'as the output.')\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (16, 1) was passed for an output of shape (None, 7, 7, 1) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "base_model = VGG19(weights='imagenet', include_top=False)\n",
    "kinship_model = siamese_model(pretrained_model=base_model)\n",
    "kinship_model.fit_generator(gen(train, train_person_to_images_map, batch_size = 16),\n",
    "                    validation_data = gen(val, val_person_to_images_map, batch_size = 16), \n",
    "                    epochs = 100, verbose = 2, \n",
    "                    steps_per_epoch = 200, \n",
    "                    validation_steps = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kinship_model_json = kinship_model.to_json()\n",
    "# with open(\"kinship_model.json\", \"w\") as json_file:\n",
    "#     json_file.write(kinship_model_json)\n",
    "    \n",
    "# kinship_model.save_weights(\"kinship_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "# def baseline_model():\n",
    "#     input_1 = Input(shape=(224, 224, 3))\n",
    "#     input_2 = Input(shape=(224, 224, 3))\n",
    "\n",
    "# #     base_model = VGGFace(model='resnet50', include_top=False)\n",
    "    \n",
    "#     base_model = VGG19(weights='imagenet', include_top=False)\n",
    "    \n",
    "#     for x in base_model.layers[:-3]:\n",
    "#         x.trainable = True\n",
    "\n",
    "#     x1 = base_model(input_1)\n",
    "#     x2 = base_model(input_2)\n",
    "\n",
    "#     # x1_ = Reshape(target_shape=(7*7, 2048))(x1)\n",
    "#     # x2_ = Reshape(target_shape=(7*7, 2048))(x2)\n",
    "#     #\n",
    "#     # x_dot = Dot(axes=[2, 2], normalize=True)([x1_, x2_])\n",
    "#     # x_dot = Flatten()(x_dot)\n",
    "\n",
    "#     x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n",
    "#     x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n",
    "\n",
    "#     x3 = Subtract()([x1, x2])\n",
    "#     x3 = Multiply()([x3, x3])\n",
    "\n",
    "#     x = Multiply()([x1, x2])\n",
    "\n",
    "#     x = Concatenate(axis=-1)([x, x3])\n",
    "\n",
    "#     x = Dense(100, activation=\"relu\")(x)\n",
    "#     x = Dropout(0.01)(x)\n",
    "#     out = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "#     model = Model([input_1, input_2], out)\n",
    "\n",
    "#     model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n",
    "\n",
    "# #     model.summary()\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = baseline_model()\n",
    "# # model.load_weights(file_path)\n",
    "# model.fit_generator(gen(train, train_person_to_images_map, batch_size=16), use_multiprocessing=True,\n",
    "#                     validation_data=gen(val, val_person_to_images_map, batch_size=16), epochs=20, verbose=2,\n",
    "#                     workers=4, steps_per_epoch=200, validation_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object gen at 0x13db494f8>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def read_img(path):\n",
    "#     img = cv2.imread(path)\n",
    "#     img = np.array(img).astype(np.float)\n",
    "#     return preprocess_input(img)\n",
    "\n",
    "\n",
    "# def gen(list_tuples, person_to_images_map, batch_size=16):\n",
    "#     ppl = list(person_to_images_map.keys())\n",
    "#     while True:\n",
    "#         batch_tuples = sample(list_tuples, batch_size // 2)\n",
    "#         labels = [1] * len(batch_tuples)\n",
    "# #         print(batch_tuples)\n",
    "# #         print(labels)\n",
    "#         while len(batch_tuples) < batch_size:\n",
    "#             p1 = choice(ppl)\n",
    "#             p2 = choice(ppl)\n",
    "# #             print('p1: ', p1)\n",
    "# #             print('p2: ', p2)\n",
    "#             if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n",
    "#                 batch_tuples.append((p1, p2))\n",
    "#                 labels.append(0)\n",
    "# #         print(batch_tuples)\n",
    "# #         print(labels)\n",
    "#         for x in batch_tuples:\n",
    "# #             print('x:, ', x)\n",
    "#             if not len(person_to_images_map[x[0]]):\n",
    "# #                 print('x[0]:, ', x)\n",
    "#                 print(x[0])\n",
    "# #         print('------------------------')\n",
    "#         X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n",
    "# #         print(len(X1))\n",
    "# #         print(X1)\n",
    "# #         print('------------------------')\n",
    "#         X1 = np.array([read_img(x) for x in X1])\n",
    "# #         print(X1)\n",
    "# #         print('------------------------')\n",
    "#         X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n",
    "#         X2 = np.array([read_img(x) for x in X2])\n",
    "        \n",
    "# #         print(type(X1))\n",
    "# #         print(type(X2))\n",
    "# #         print([X1,X2])\n",
    "#         yield [X1, X2], labels\n",
    "\n",
    "\n",
    "# gen(train, train_person_to_images_map, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen(train, train_person_to_images_map, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen(list_tuples, person_to_images_map, batch_size=16):\n",
    "#     ppl = list(person_to_images_map.keys())\n",
    "#     while True:\n",
    "#         batch_tuples = sample(list_tuples, batch_size // 2)\n",
    "#         labels = [1] * len(batch_tuples)\n",
    "#         while len(batch_tuples) < batch_size:\n",
    "#             p1 = choice(ppl)\n",
    "#             p2 = choice(ppl)\n",
    "\n",
    "#             if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n",
    "#                 batch_tuples.append((p1, p2))\n",
    "#                 labels.append(0)\n",
    "\n",
    "#         for x in batch_tuples:\n",
    "#             if not len(person_to_images_map[x[0]]):\n",
    "#                 print(x[0])\n",
    "\n",
    "#         X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n",
    "#         X1 = np.array([read_img(x) for x in X1])\n",
    "\n",
    "#         X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n",
    "#         X2 = np.array([read_img(x) for x in X2])\n",
    "        \n",
    "# #         print(X1)\n",
    "        \n",
    "#         yield [X1, X2], labels\n",
    "        \n",
    "# # gen(train, train_person_to_images_map, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install opencv-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
